{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title",
    "tags": []
   },
   "source": [
    "# Getting started with Superwise.ai on GCP Vertex AI\n",
    "\n",
    "In this notebook, we will demonstrate how to integrate a Vertex AI based development workflow with Superwise.ai\n",
    "\n",
    "**Part I** of this notebook walks you through building a classical model for predicting the Titanic passenger survival, using Sci-kit learn on Vertex AI. \n",
    "\n",
    "It is based on [GCP tutorial for building custom models on Vertex AI](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb).\n",
    "\n",
    "**Part II** of this notebook will walk you through how to setup Superwise.ai to start tracking your model, by registering and providing a baseline for the model's behavior.\n",
    "\n",
    "**Part III** will demonstrate how to send new predictions from your model to Superwise.ai, simulating a post-deployment scenario.\n",
    "\n",
    "At this point, you should be able to start seeing insights from Superwise.ai in the web portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. A Superwise.ai account that enables you to login and view insights\n",
    "2. A set of API keys for sending data to Superwise.ai \n",
    "3. Permissions to create models, training jobs and inference endpoints inside Vertex.ai\n",
    "4. Grant Superwise.ai permissions to your GCS bucket #soon to be removed\n",
    "\n",
    "Note: this notebook works best when run from within a Vertex AI notebook instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:custom"
   },
   "source": [
    "## Part I - building a Vertex AI Model to predict the survival chances of the Titanic passengers\n",
    "\n",
    "This is a classical SVM model, over a publicly available dataset.\n",
    "\n",
    "This guide is based on the best practices from [Vertex AI's example for building a Scikit-Learn model.](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Install the latest version of Vertex AI SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1fd00fa70a2a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YsxCgt1zlugo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.5.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (21.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in ./.local/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in ./.local/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.42.3)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.31.2)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.19.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.53.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (58.0.4)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.35.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.18.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.38.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.0.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.0.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_storage"
   },
   "source": [
    "Install the latest version of *google-cloud-storage* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qssss-KSlugo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (1.42.2)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.35.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.0.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.0.3)\n",
      "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.31.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (3.18.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (21.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (2021.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (58.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.5.30)\n",
      "Installing collected packages: google-cloud-storage\n",
      "Successfully installed google-cloud-storage-1.42.3\n"
     ]
    }
   ],
   "source": [
    "! pip install {USER_FLAG} -U google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3hYaR6gLEK-"
   },
   "source": [
    "Install the latest version of *google-cloud-bigquery* library as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed everything, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "bzPxhxS5lugp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you might be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  pinhasi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = shell_output[0]\n",
    "print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_project_id"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "USd_pUT0lugr"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step.\n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vF60K5v1lugs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a training job using the Cloud SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
    "the code from this package. In this tutorial, Vertex AI also saves the\n",
    "trained model that results from your job in the same bucket. Using this model artifact, you can then\n",
    "create Vertex AI model and endpoint resources in order to serve\n",
    "online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = f\"{PROJECT_ID}-superwise-vertex-demo-bucket\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz8J0vmSlugt"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oadE10x2lugu"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_aip",
    "tags": []
   },
   "source": [
    "### Import Vertex SDK for Python\n",
    "\n",
    "Import the Vertex SDK for Python into your Python environment and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cNEiwLd0lugu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "## Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "### Set pre-built containers\n",
    "\n",
    "Vertex AI provides pre-built containers to run training and prediction.\n",
    "\n",
    "For the latest list, see [Pre-built containers for training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) and [Pre-built containers for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1u1mr18jlugv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VERSION = \"scikit-learn-cpu.0-23\"\n",
    "DEPLOY_VERSION = \"sklearn-cpu.0-23\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "### Set machine types\n",
    "\n",
    "Next, set the machine types to use for training and prediction.\n",
    "\n",
    "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure your compute resources for training and prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*.\n",
    "\n",
    "Learn [which machine types are available for training](https://cloud.google.com/vertex-ai/docs/training/configure-compute) and [which machine types are available for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YAXwbqKKlugv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-2\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"2\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59f24e7d2269"
   },
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_FEATURES = [\n",
    "    'sex']\n",
    "\n",
    "# List all column names for numeric features\n",
    "NUMERIC_FEATURES = [\n",
    "    'age',\n",
    "    'fare']\n",
    "\n",
    "# List all column names for categorical features\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'pclass',\n",
    "    'embarked',\n",
    "    'home_dest',\n",
    "    'parch',\n",
    "    'sibsp']\n",
    "\n",
    "LABEL = ['survived']\n",
    "\n",
    "ALL_COLUMNS = BINARY_FEATURES+NUMERIC_FEATURES+CATEGORICAL_FEATURES+LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "890d562c6291"
   },
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "df = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')\n",
    "df = df.rename(columns={\"home.dest\" : \"home_dest\"})\n",
    "df = df[ALL_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>pclass</th>\n",
       "      <th>embarked</th>\n",
       "      <th>home_dest</th>\n",
       "      <th>parch</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>29</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>St Louis, MO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.55</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>151.55</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>151.55</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>151.55</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>female</td>\n",
       "      <td>14.5</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>female</td>\n",
       "      <td>?</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>male</td>\n",
       "      <td>26.5</td>\n",
       "      <td>7.225</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>7.225</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>male</td>\n",
       "      <td>29</td>\n",
       "      <td>7.875</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sex     age      fare  pclass embarked  \\\n",
       "0     female      29  211.3375       1        S   \n",
       "1       male  0.9167    151.55       1        S   \n",
       "2     female       2    151.55       1        S   \n",
       "3       male      30    151.55       1        S   \n",
       "4     female      25    151.55       1        S   \n",
       "...      ...     ...       ...     ...      ...   \n",
       "1304  female    14.5   14.4542       3        C   \n",
       "1305  female       ?   14.4542       3        C   \n",
       "1306    male    26.5     7.225       3        C   \n",
       "1307    male      27     7.225       3        C   \n",
       "1308    male      29     7.875       3        S   \n",
       "\n",
       "                            home_dest  parch  sibsp  survived  \n",
       "0                        St Louis, MO      0      0         1  \n",
       "1     Montreal, PQ / Chesterville, ON      2      1         1  \n",
       "2     Montreal, PQ / Chesterville, ON      2      1         0  \n",
       "3     Montreal, PQ / Chesterville, ON      2      1         0  \n",
       "4     Montreal, PQ / Chesterville, ON      2      1         0  \n",
       "...                               ...    ...    ...       ...  \n",
       "1304                                ?      0      1         0  \n",
       "1305                                ?      0      1         0  \n",
       "1306                                ?      0      0         0  \n",
       "1307                                ?      0      0         0  \n",
       "1308                                ?      0      0         0  \n",
       "\n",
       "[1309 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing_numerics(df: pd.DataFrame, numeric_columns):\n",
    "    '''\n",
    "    removes invalid values in the numeric columns\n",
    "\n",
    "            Parameters:\n",
    "                    df (pandas.DataFrame): The Pandas Dataframe to alter\n",
    "                    numeric_columns (List[str]): List of column names that are numberic from the DataFrame\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the numeric columns fixed\n",
    "    '''\n",
    "\n",
    "    for n in numeric_columns:\n",
    "        df[n] = pd.to_numeric(df[n], errors='coerce')\n",
    "\n",
    "    df = df.fillna(df.mean())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_missing_numerics(df, NUMERIC_FEATURES)\n",
    "# add a record_id column, using the dataframe's natural index. This is needed for training so that later we can send the ID as part of the prediction payload\n",
    "df = df.reset_index().rename(columns = {'index': 'record_id'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split and store as CSV files in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=\"survived\")\n",
    "y = df[\"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "train = X_train.copy()\n",
    "train[\"survived\"] = y_train\n",
    "\n",
    "test = X_test.copy()\n",
    "test[\"survived\"] = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(f\"gs://{BUCKET_NAME}/data/titanic_train.csv\")\n",
    "test.to_csv(f\"gs://{BUCKET_NAME}/data/titanic_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training code package\n",
    "\n",
    "For this tutorial, we will wrap our training script in a package.\n",
    "This package can be run locally or installed inside the training container when running on the Vertex AI training machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p titanic/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting titanic/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile titanic/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'gcsfs==0.7.1',\n",
    "    'dask[dataframe]==2021.2.0',\n",
    "    'google-cloud-bigquery-storage==1.0.0',\n",
    "    'six==1.15.0'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(), # Automatically find packages within this directory or below.\n",
    "    include_package_data=True, # if packages include any data files, those will be packed together.\n",
    "    description='Classification training titanic survivors prediction model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch titanic/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting titanic/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile titanic/trainer/task.py\n",
    "\n",
    "from google.cloud import bigquery, bigquery_storage, storage\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from typing import Union, List\n",
    "import os, logging, json, pickle, argparse\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# feature selection.  The FEATURE list defines what features are needed from the training data.\n",
    "# as well as the types of those features. We will perform different feature engineering depending on the type\n",
    "\n",
    "# List all column names for binary features: 0,1 or True,False or Male,Female etc\n",
    "BINARY_FEATURES = [\n",
    "    'sex']\n",
    "\n",
    "# List all column names for numeric features\n",
    "NUMERIC_FEATURES = [\n",
    "    'age',\n",
    "    'fare']\n",
    "\n",
    "# List all column names for categorical features\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'pclass',\n",
    "    'embarked',\n",
    "    'home_dest',\n",
    "    'parch',\n",
    "    'sibsp']\n",
    "\n",
    "# ID column - needed to support predict() over numpy arrays \n",
    "ID = ['record_id']\n",
    "\n",
    "ALL_COLUMNS = ID + BINARY_FEATURES+NUMERIC_FEATURES+CATEGORICAL_FEATURES \n",
    "\n",
    "# define the column name for label\n",
    "LABEL = 'survived'\n",
    "\n",
    "\n",
    "# Define the index position of each feature. This is needed for processing a\n",
    "# numpy array (instead of pandas) which has no column names.\n",
    "BINARY_FEATURES_IDX = list(range(1,len(BINARY_FEATURES)+1))\n",
    "NUMERIC_FEATURES_IDX = list(range(len(BINARY_FEATURES)+1, len(BINARY_FEATURES)+len(NUMERIC_FEATURES)+1))\n",
    "CATEGORICAL_FEATURES_IDX = list(range(len(BINARY_FEATURES+NUMERIC_FEATURES)+1, len(ALL_COLUMNS)))\n",
    "\n",
    "\n",
    "def load_data_from_gcs(data_gcs_path: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Loads data from Google Cloud Storage (GCS) to a dataframe\n",
    "\n",
    "            Parameters:\n",
    "                    data_gcs_path (str): gs path for the location of the data. Wildcards are also supported. i.e gs://example_bucket/data/training-*.csv\n",
    "\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the data from GCP loaded\n",
    "    '''\n",
    "\n",
    "    # using dask that supports wildcards to read multiple files. Then with dd.read_csv().compute we create a pandas dataframe\n",
    "    # Additionally I have noticed that some values for TotalCharges are missing and this creates confusion regarding TotalCharges the data types.\n",
    "    # to overcome this we manually define TotalCharges as object.\n",
    "    # We will later fix this upnormality\n",
    "    logging.info(\"reading gs data: {}\".format(data_gcs_path))\n",
    "    return dd.read_csv(data_gcs_path, dtype={'TotalCharges': 'object'}).compute()\n",
    "\n",
    "\n",
    "def load_data_from_bq(bq_uri: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Loads data from BigQuery table (BQ) to a dataframe\n",
    "\n",
    "            Parameters:\n",
    "                    bq_uri (str): bq table uri. i.e: example_project.example_dataset.example_table\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the data from GCP loaded\n",
    "    '''\n",
    "    if not bq_uri.startswith('bq://'):\n",
    "        raise Exception(\"uri is not a BQ uri. It should be bq://project_id.dataset.table\")\n",
    "    logging.info(\"reading bq data: {}\".format(bq_uri))\n",
    "    project,dataset,table =  bq_uri.split(\".\")\n",
    "    bqclient = bigquery.Client(project=project[5:])\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    query_string = \"\"\"\n",
    "    SELECT * from {ds}.{tbl}\n",
    "    \"\"\".format(ds=dataset, tbl=table)\n",
    "\n",
    "    return (\n",
    "        bqclient.query(query_string)\n",
    "            .result()\n",
    "            .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "\n",
    "def clean_missing_numerics(df: pd.DataFrame, numeric_columns):\n",
    "    '''\n",
    "    removes invalid values in the numeric columns\n",
    "\n",
    "            Parameters:\n",
    "                    df (pandas.DataFrame): The Pandas Dataframe to alter\n",
    "                    numeric_columns (List[str]): List of column names that are numberic from the DataFrame\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the numeric columns fixed\n",
    "    '''\n",
    "\n",
    "    for n in numeric_columns:\n",
    "        df[n] = pd.to_numeric(df[n], errors='coerce')\n",
    "\n",
    "    df = df.fillna(df.mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_selection(df: pd.DataFrame, selected_columns: List[str], label_column: str) -> (pd.DataFrame, pd.Series):\n",
    "    '''\n",
    "    From a dataframe it creates a new dataframe with only selected columns and returns it.\n",
    "    Additionally it splits the label column into a pandas Series.\n",
    "\n",
    "            Parameters:\n",
    "                    df (pandas.DataFrame): The Pandas Dataframe to drop columns and extract label\n",
    "                    selected_columns (List[str]): List of strings with the selected columns. i,e ['col_1', 'col_2', ..., 'col_n' ]\n",
    "                    label_column (str): The name of the label column\n",
    "\n",
    "            Returns:\n",
    "                    tuple(pandas.DataFrame, pandas.Series): Tuble with the new pandas DataFrame containing only selected columns and lablel pandas Series\n",
    "    '''\n",
    "    # We create a series with the prediciton label\n",
    "    labels = df[label_column]\n",
    "\n",
    "    data = df.loc[:, selected_columns]\n",
    "\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def pipeline_builder(params_svm: dict, bin_ftr_idx: List[int], num_ftr_idx: List[int], cat_ftr_idx: List[int]) -> Pipeline:\n",
    "    '''\n",
    "    Builds a sklearn pipeline with preprocessing and model configuration.\n",
    "    Preprocessing steps are:\n",
    "        * OrdinalEncoder - used for binary features\n",
    "        * StandardScaler - used for numerical features\n",
    "        * OneHotEncoder - used for categorical features\n",
    "    Model used is SVC\n",
    "\n",
    "            Parameters:\n",
    "                    params_svm (dict): List of parameters for the sklearn.svm.SVC classifier\n",
    "                    bin_ftr_idx (List[str]): List of ints that mark the column indexes with binary columns. i.e [0, 2, ... , X ]\n",
    "                    num_ftr_idx (List[str]): List of ints that mark the column indexes with numerica columns. i.e [6, 3, ... , X ]\n",
    "                    cat_ftr_idx (List[str]): List of ints that mark the column indexes with categorical columns. i.e [5, 10, ... , X ]\n",
    "                    label_column (str): The name of the label column\n",
    "\n",
    "            Returns:\n",
    "                     Pipeline: sklearn.pipelines.Pipeline with preprocessing and model training\n",
    "    '''\n",
    "\n",
    "    # Definining a preprocessing step for our pipeline.\n",
    "    # it specifies how the features are going to be transformed\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('bin', OrdinalEncoder(), bin_ftr_idx),\n",
    "            ('num', StandardScaler(), num_ftr_idx),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'),  cat_ftr_idx)], remainder='drop', n_jobs=-1)\n",
    "\n",
    "\n",
    "    # We now create a full pipeline, for preprocessing and training.\n",
    "    # for training we selected a linear SVM classifier\n",
    "\n",
    "    clf = SVC()\n",
    "    clf.set_params(**params_svm)\n",
    "\n",
    "    return Pipeline(steps=[ ('preprocessor', preprocessor),\n",
    "                            ('classifier', clf)])\n",
    "\n",
    "def train_pipeline(clf: Pipeline, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "    '''\n",
    "    Trains a sklearn pipeline by fiting training data an labels and returns the accuracy f1 score\n",
    "\n",
    "            Parameters:\n",
    "                    clf (sklearn.pipelines.Pipeline): the Pipeline object to fit the data\n",
    "                    X: (pd.DataFrame OR np.ndarray): Training vectors of shape n_samples x n_features, where n_samples is the number of samples and n_features is the number of features.\n",
    "                    y: (pd.DataFrame OR np.ndarray): Labels of shape n_samples. Order should mathc Training Vectors X\n",
    "\n",
    "            Returns:\n",
    "                    score (float): Average F1 score from all cross validations\n",
    "    '''\n",
    "    # run cross validation to get training score. we can use this score to optimise training\n",
    "    score = cross_val_score(clf, X, y, cv=10, n_jobs=-1).mean()\n",
    "\n",
    "    # Now we fit all our data to the classifier.\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return score\n",
    "\n",
    "def process_gcs_uri(uri: str) -> (str, str, str, str):\n",
    "    '''\n",
    "    Receives a Google Cloud Storage (GCS) uri and breaks it down to the scheme, bucket, path and file\n",
    "\n",
    "            Parameters:\n",
    "                    uri (str): GCS uri\n",
    "\n",
    "            Returns:\n",
    "                    scheme (str): uri scheme\n",
    "                    bucket (str): uri bucket\n",
    "                    path (str): uri path\n",
    "                    file (str): uri file\n",
    "    '''\n",
    "    url_arr = uri.split(\"/\")\n",
    "    if \".\" not in url_arr[-1]:\n",
    "        file = \"\"\n",
    "    else:\n",
    "        file = url_arr.pop()\n",
    "    scheme = url_arr[0]\n",
    "    bucket = url_arr[2]\n",
    "    path = \"/\".join(url_arr[3:])\n",
    "    path = path[:-1] if path.endswith(\"/\") else path\n",
    "\n",
    "    return scheme, bucket, path, file\n",
    "\n",
    "def pipeline_export_gcs(fitted_pipeline: Pipeline, model_dir: str) -> str:\n",
    "    '''\n",
    "    Exports trained pipeline to GCS\n",
    "\n",
    "            Parameters:\n",
    "                    fitted_pipeline (sklearn.pipelines.Pipeline): the Pipeline object with data already fitted (trained pipeline object)\n",
    "                    model_dir (str): GCS path to store the trained pipeline. i.e gs://example_bucket/training-job\n",
    "            Returns:\n",
    "                    export_path (str): Model GCS location\n",
    "    '''\n",
    "    scheme, bucket, path, file = process_gcs_uri(model_dir)\n",
    "    if scheme != \"gs:\":\n",
    "        raise ValueError(\"URI scheme must be gs\")\n",
    "\n",
    "    # Upload the model to GCS\n",
    "    b = storage.Client().bucket(bucket)\n",
    "    export_path = os.path.join(path, 'model.pkl')\n",
    "    blob = b.blob(export_path)\n",
    "\n",
    "    blob.upload_from_string(pickle.dumps(fitted_pipeline))\n",
    "    return scheme + \"//\" + os.path.join(bucket, export_path)\n",
    "\n",
    "\n",
    "def prepare_report(cv_score: float, model_params: dict, classification_report: str, columns: List[str], example_data: np.ndarray) -> str:\n",
    "    '''\n",
    "    Prepares a training report in Text\n",
    "\n",
    "            Parameters:\n",
    "                    cv_score (float): score of the training job during cross validation of training data\n",
    "                    model_params (dict): dictonary containing the parameters the model was trained with\n",
    "                    classification_report (str): Model classification report with test data\n",
    "                    columns (List[str]): List of columns that where used in training.\n",
    "                    example_data (np.array): Sample of data (2-3 rows are enough). This is used to include what the prediciton payload should look like for the model\n",
    "            Returns:\n",
    "                    report (str): Full report in text\n",
    "    '''\n",
    "\n",
    "    buffer_example_data = '['\n",
    "    for r in example_data:\n",
    "        buffer_example_data+='['\n",
    "        for c in r:\n",
    "            if(isinstance(c,str)):\n",
    "                buffer_example_data+=\"'\"+c+\"', \"\n",
    "            else:\n",
    "                buffer_example_data+=str(c)+\", \"\n",
    "        buffer_example_data= buffer_example_data[:-2]+\"], \\n\"\n",
    "    buffer_example_data= buffer_example_data[:-3]+\"]\"\n",
    "\n",
    "    report = \"\"\"\n",
    "Training Job Report    \n",
    "    \n",
    "Cross Validation Score: {cv_score}\n",
    "\n",
    "Training Model Parameters: {model_params}\n",
    "    \n",
    "Test Data Classification Report:\n",
    "{classification_report}\n",
    "\n",
    "Example of data array for prediciton:\n",
    "\n",
    "Order of columns:\n",
    "{columns}\n",
    "\n",
    "Example for clf.predict()\n",
    "{predict_example}\n",
    "\n",
    "\n",
    "Example of GCP API request body:\n",
    "{{\n",
    "    \"instances\": {json_example}\n",
    "}}\n",
    "\n",
    "\"\"\".format(\n",
    "        cv_score=cv_score,\n",
    "        model_params=json.dumps(model_params),\n",
    "        classification_report=classification_report,\n",
    "        columns = columns,\n",
    "        predict_example = buffer_example_data,\n",
    "        json_example = json.dumps(example_data.tolist()))\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def report_export_gcs(report: str, report_dir: str) -> None:\n",
    "    '''\n",
    "    Exports training job report to GCS\n",
    "\n",
    "            Parameters:\n",
    "                    report (str): Full report in text to sent to GCS\n",
    "                    report_dir (str): GCS path to store the report model. i.e gs://example_bucket/training-job\n",
    "            Returns:\n",
    "                    export_path (str): Report GCS location\n",
    "    '''\n",
    "    scheme, bucket, path, file = process_gcs_uri(report_dir)\n",
    "    if scheme != \"gs:\":\n",
    "        raise ValueError(\"URI scheme must be gs\")\n",
    "\n",
    "    # Upload the model to GCS\n",
    "    b = storage.Client().bucket(bucket)\n",
    "\n",
    "    export_path = os.path.join(path, 'report.txt')\n",
    "    blob = b.blob(export_path)\n",
    "\n",
    "    blob.upload_from_string(report)\n",
    "\n",
    "    return scheme + \"//\" + os.path.join(bucket, export_path)\n",
    "\n",
    "\n",
    "\n",
    "# Define all the command line arguments your model can accept for training\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--model_param_kernel',\n",
    "        help = 'SVC model parameter- kernel',\n",
    "        choices=['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        type = str,\n",
    "        default = 'linear'\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--model_param_degree',\n",
    "        help = 'SVC model parameter- Degree. Only applies for poly kernel',\n",
    "        type = int,\n",
    "        default = 3\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--model_param_C',\n",
    "        help = 'SVC model parameter- C (regularization)',\n",
    "        type = float,\n",
    "        default = 1.0\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--model_param_probability',\n",
    "        help = 'Whether to enable probability estimates',\n",
    "        type = bool,\n",
    "        default = True\n",
    "    )\n",
    "\n",
    "\n",
    "    ''' \n",
    "    Vertex AI automatically populates a set of environment varialbes in the container that executes \n",
    "    your training job. those variables include:\n",
    "        * AIP_MODEL_DIR - Directory selected as model dir\n",
    "        * AIP_DATA_FORMAT - Type of dataset selected for training (can be csv or bigquery)\n",
    "    \n",
    "    Vertex AI will automatically split selected dataset into training,validation and testing\n",
    "    and 3 more environment variables will reflect the locaiton of the data:\n",
    "        * AIP_TRAINING_DATA_URI - URI of Training data\n",
    "        * AIP_VALIDATION_DATA_URI - URI of Validation data\n",
    "        * AIP_TEST_DATA_URI - URI of Test data\n",
    "        \n",
    "    Notice that those environment varialbes are default. If the user provides a value using CLI argument,\n",
    "    the environment variable will be ignored. If the user does not provide anything as CLI  argument\n",
    "    the program will try and use the environemnt variables if those exist. otherwise will leave empty.\n",
    "    '''\n",
    "    parser.add_argument(\n",
    "        '--model_dir',\n",
    "        help = 'Directory to output model and artifacts',\n",
    "        type = str,\n",
    "        default = os.environ['AIP_MODEL_DIR'] if 'AIP_MODEL_DIR' in os.environ else \"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--data_format',\n",
    "        choices=['csv', 'bigquery'],\n",
    "        help = 'format of data uri csv for gs:// paths and bigquery for project.dataset.table formats',\n",
    "        type = str,\n",
    "        default =  os.environ['AIP_DATA_FORMAT'] if 'AIP_DATA_FORMAT' in os.environ else \"csv\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--training_data_uri',\n",
    "        help = 'location of training data in either gs:// uri or bigquery uri',\n",
    "        type = str,\n",
    "        default =  os.environ['AIP_TRAINING_DATA_URI'] if 'AIP_TRAINING_DATA_URI' in os.environ else \"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--validation_data_uri',\n",
    "        help = 'location of validation data in either gs:// uri or bigquery uri',\n",
    "        type = str,\n",
    "        default =  os.environ['AIP_VALIDATION_DATA_URI'] if 'AIP_VALIDATION_DATA_URI' in os.environ else \"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test_data_uri',\n",
    "        help = 'location of test data in either gs:// uri or bigquery uri',\n",
    "        type = str,\n",
    "        default =  os.environ['AIP_TEST_DATA_URI'] if 'AIP_TEST_DATA_URI' in os.environ else \"\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"-v\", \"--verbose\", help=\"increase output verbosity\",\n",
    "                        action=\"store_true\")\n",
    "\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "\n",
    "    if args.verbose:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "    logging.info('Model artifacts will be exported here: {}'.format(arguments['model_dir']))\n",
    "    logging.info('Data format: {}'.format(arguments[\"data_format\"]))\n",
    "    logging.info('Training data uri: {}'.format(arguments['training_data_uri']) )\n",
    "    logging.info('Validation data uri: {}'.format(arguments['validation_data_uri']))\n",
    "    logging.info('Test data uri: {}'.format(arguments['test_data_uri']))\n",
    "\n",
    "\n",
    "    '''\n",
    "    We have 2 different ways to load our data to pandas. One is from cloud storage by loading csv files and\n",
    "    the other is by connecting to BigQuery. Vertex AI supports both and \n",
    "    here we created a code that depelnding on the dataset provided, we will select the appropriated loading method.\n",
    "    '''\n",
    "    logging.info('Loading {} data'.format(arguments[\"data_format\"]))\n",
    "    if(arguments['data_format']=='csv'):\n",
    "        df_train = load_data_from_gcs(arguments['training_data_uri'])\n",
    "        df_test = load_data_from_gcs(arguments['test_data_uri'])\n",
    "        df_valid = load_data_from_gcs(arguments['validation_data_uri'])\n",
    "    elif(arguments['data_format']=='bigquery'):\n",
    "        print(arguments['training_data_uri'])\n",
    "        df_train = load_data_from_bq(arguments['training_data_uri'])\n",
    "        df_test = load_data_from_bq(arguments['test_data_uri'])\n",
    "        df_valid = load_data_from_bq(arguments['validation_data_uri'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data type \")\n",
    "\n",
    "    #as we will be using cross validation, we will have just a training set and a single test set.\n",
    "    # we ill merge the test and validation to achieve an 80%-20% split\n",
    "    df_test = pd.concat([df_test,df_valid])\n",
    "\n",
    "    logging.info('Defining model parameters')\n",
    "    model_params = dict()\n",
    "    model_params['kernel'] = arguments['model_param_kernel']\n",
    "    model_params['degree'] = arguments['model_param_degree']\n",
    "    model_params['C'] = arguments['model_param_C']\n",
    "    model_params['probability'] = arguments['model_param_probability']\n",
    "\n",
    "    df_train = clean_missing_numerics(df_train, NUMERIC_FEATURES)\n",
    "    df_test = clean_missing_numerics(df_test, NUMERIC_FEATURES)\n",
    "\n",
    "\n",
    "    logging.info('Running feature selection')\n",
    "    X_train, y_train = data_selection(df_train, ALL_COLUMNS, LABEL)\n",
    "    X_test, y_test = data_selection(df_test, ALL_COLUMNS, LABEL)\n",
    "\n",
    "    logging.info('Training pipelines in CV')\n",
    "    clf = pipeline_builder(model_params, BINARY_FEATURES_IDX, NUMERIC_FEATURES_IDX, CATEGORICAL_FEATURES_IDX)\n",
    "\n",
    "    cv_score = train_pipeline(clf, X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    logging.info('Export trained pipeline and report')\n",
    "    pipeline_export_gcs(clf, arguments['model_dir'])\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "    test_score = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "    logging.info('f1score: '+ str(test_score))\n",
    "\n",
    "    report = prepare_report(cv_score,\n",
    "                            model_params,\n",
    "                            classification_report(y_test,y_pred),\n",
    "                            ALL_COLUMNS,\n",
    "                            X_test.to_numpy()[0:2])\n",
    "\n",
    "    report_export_gcs(report, arguments['model_dir'])\n",
    "\n",
    "\n",
    "    logging.info('Training job completed. Exiting...')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the training package locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "copying trainer/task.py -> build/lib/trainer\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/trainer\n",
      "copying build/lib/trainer/__init__.py -> build/bdist.linux-x86_64/egg/trainer\n",
      "copying build/lib/trainer/task.py -> build/bdist.linux-x86_64/egg/trainer\n",
      "byte-compiling build/bdist.linux-x86_64/egg/trainer/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/trainer/task.py to task.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying trainer.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying trainer.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying trainer.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying trainer.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying trainer.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/trainer-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing trainer-0.1-py3.7.egg\n",
      "Removing /opt/conda/lib/python3.7/site-packages/trainer-0.1-py3.7.egg\n",
      "Copying trainer-0.1-py3.7.egg to /opt/conda/lib/python3.7/site-packages\n",
      "trainer 0.1 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /opt/conda/lib/python3.7/site-packages/trainer-0.1-py3.7.egg\n",
      "Processing dependencies for trainer==0.1\n",
      "Searching for six==1.15.0\n",
      "Best match: six 1.15.0\n",
      "Processing six-1.15.0-py3.7.egg\n",
      "six 1.15.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages/six-1.15.0-py3.7.egg\n",
      "Searching for google-cloud-bigquery-storage==1.0.0\n",
      "Best match: google-cloud-bigquery-storage 1.0.0\n",
      "Processing google_cloud_bigquery_storage-1.0.0-py3.7.egg\n",
      "google-cloud-bigquery-storage 1.0.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages/google_cloud_bigquery_storage-1.0.0-py3.7.egg\n",
      "Searching for dask==2021.2.0\n",
      "Best match: dask 2021.2.0\n",
      "Adding dask 2021.2.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for gcsfs==0.7.1\n",
      "Best match: gcsfs 0.7.1\n",
      "Processing gcsfs-0.7.1-py3.7.egg\n",
      "gcsfs 0.7.1 is already the active version in easy-install.pth\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages/gcsfs-0.7.1-py3.7.egg\n",
      "Searching for google-api-core==1.31.2\n",
      "Best match: google-api-core 1.31.2\n",
      "Adding google-api-core 1.31.2 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for numpy==1.19.5\n",
      "Best match: numpy 1.19.5\n",
      "Adding numpy 1.19.5 to easy-install.pth file\n",
      "Installing f2py script to /opt/conda/bin\n",
      "Installing f2py3 script to /opt/conda/bin\n",
      "Installing f2py3.7 script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for toolz==0.11.1\n",
      "Best match: toolz 0.11.1\n",
      "Processing toolz-0.11.1-py3.7.egg\n",
      "toolz 0.11.1 is already the active version in easy-install.pth\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages/toolz-0.11.1-py3.7.egg\n",
      "Searching for partd==1.2.0\n",
      "Best match: partd 1.2.0\n",
      "Processing partd-1.2.0-py3.7.egg\n",
      "partd 1.2.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages/partd-1.2.0-py3.7.egg\n",
      "Searching for fsspec==2021.8.1\n",
      "Best match: fsspec 2021.8.1\n",
      "Adding fsspec 2021.8.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for pandas==1.3.3\n",
      "Best match: pandas 1.3.3\n",
      "Adding pandas 1.3.3 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for PyYAML==5.4.1\n",
      "Best match: PyYAML 5.4.1\n",
      "Adding PyYAML 5.4.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for requests==2.25.1\n",
      "Best match: requests 2.25.1\n",
      "Adding requests 2.25.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for google-auth==1.35.0\n",
      "Best match: google-auth 1.35.0\n",
      "Adding google-auth 1.35.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for google-auth-oauthlib==0.4.6\n",
      "Best match: google-auth-oauthlib 0.4.6\n",
      "Adding google-auth-oauthlib 0.4.6 to easy-install.pth file\n",
      "Installing google-oauthlib-tool script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for decorator==5.1.0\n",
      "Best match: decorator 5.1.0\n",
      "Adding decorator 5.1.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for aiohttp==3.7.4.post0\n",
      "Best match: aiohttp 3.7.4.post0\n",
      "Adding aiohttp 3.7.4.post0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for grpcio==1.38.1\n",
      "Best match: grpcio 1.38.1\n",
      "Adding grpcio 1.38.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for googleapis-common-protos==1.53.0\n",
      "Best match: googleapis-common-protos 1.53.0\n",
      "Adding googleapis-common-protos 1.53.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for protobuf==3.18.0\n",
      "Best match: protobuf 3.18.0\n",
      "Adding protobuf 3.18.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for packaging==21.0\n",
      "Best match: packaging 21.0\n",
      "Adding packaging 21.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for pytz==2021.1\n",
      "Best match: pytz 2021.1\n",
      "Adding pytz 2021.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for setuptools==58.0.4\n",
      "Best match: setuptools 58.0.4\n",
      "Adding setuptools 58.0.4 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for locket==0.2.1\n",
      "Best match: locket 0.2.1\n",
      "Processing locket-0.2.1-py3.7.egg\n",
      "locket 0.2.1 is already the active version in easy-install.pth\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages/locket-0.2.1-py3.7.egg\n",
      "Searching for python-dateutil==2.8.2\n",
      "Best match: python-dateutil 2.8.2\n",
      "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for certifi==2021.5.30\n",
      "Best match: certifi 2021.5.30\n",
      "Adding certifi 2021.5.30 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for idna==2.10\n",
      "Best match: idna 2.10\n",
      "Adding idna 2.10 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for chardet==4.0.0\n",
      "Best match: chardet 4.0.0\n",
      "Adding chardet 4.0.0 to easy-install.pth file\n",
      "Installing chardetect script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for urllib3==1.26.6\n",
      "Best match: urllib3 1.26.6\n",
      "Adding urllib3 1.26.6 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for rsa==4.7.2\n",
      "Best match: rsa 4.7.2\n",
      "Adding rsa 4.7.2 to easy-install.pth file\n",
      "Installing pyrsa-decrypt script to /opt/conda/bin\n",
      "Installing pyrsa-encrypt script to /opt/conda/bin\n",
      "Installing pyrsa-keygen script to /opt/conda/bin\n",
      "Installing pyrsa-priv2pub script to /opt/conda/bin\n",
      "Installing pyrsa-sign script to /opt/conda/bin\n",
      "Installing pyrsa-verify script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for pyasn1-modules==0.2.8\n",
      "Best match: pyasn1-modules 0.2.8\n",
      "Processing pyasn1_modules-0.2.8-py3.7.egg\n",
      "pyasn1-modules 0.2.8 is already the active version in easy-install.pth\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages/pyasn1_modules-0.2.8-py3.7.egg\n",
      "Searching for cachetools==4.2.2\n",
      "Best match: cachetools 4.2.2\n",
      "Adding cachetools 4.2.2 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for requests-oauthlib==1.3.0\n",
      "Best match: requests-oauthlib 1.3.0\n",
      "Adding requests-oauthlib 1.3.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for typing-extensions==3.10.0.2\n",
      "Best match: typing-extensions 3.10.0.2\n",
      "Adding typing-extensions 3.10.0.2 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for yarl==1.6.3\n",
      "Best match: yarl 1.6.3\n",
      "Adding yarl 1.6.3 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for multidict==5.1.0\n",
      "Best match: multidict 5.1.0\n",
      "Adding multidict 5.1.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for attrs==21.2.0\n",
      "Best match: attrs 21.2.0\n",
      "Adding attrs 21.2.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for async-timeout==3.0.1\n",
      "Best match: async-timeout 3.0.1\n",
      "Adding async-timeout 3.0.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for pyparsing==2.4.7\n",
      "Best match: pyparsing 2.4.7\n",
      "Adding pyparsing 2.4.7 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for pyasn1==0.4.8\n",
      "Best match: pyasn1 0.4.8\n",
      "Adding pyasn1 0.4.8 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Searching for oauthlib==3.1.1\n",
      "Best match: oauthlib 3.1.1\n",
      "Adding oauthlib 3.1.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.7/site-packages\n",
      "Finished processing dependencies for trainer==0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! cd titanic && python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "### Train the model locally\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMDARGS = [f\"--model_param_kernel=linear\", \\\n",
    "           f\"--data_format=csv\", \\\n",
    "           f\"--training_data_uri=gs://{BUCKET_NAME}/data/titanic_train.csv\", \\\n",
    "           f\"--test_data_uri=gs://{BUCKET_NAME}/data/titanic_test.csv\", \\\n",
    "           f\"--validation_data_uri=gs://{BUCKET_NAME}/data/titanic_test.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a specific path to write the model file to the args\n",
    "CMDARGS_LOCAL = \" \".join(CMDARGS + [f\"--model_dir=gs://{BUCKET_NAME}/titanic/trial\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Model artifacts will be exported here: gs://pinhasi-superwise-vertex-demo-bucket/titanic/trial\n",
      "INFO:root:Data format: csv\n",
      "INFO:root:Training data uri: gs://pinhasi-superwise-vertex-demo-bucket/data/titanic_train.csv\n",
      "INFO:root:Validation data uri: gs://pinhasi-superwise-vertex-demo-bucket/data/titanic_test.csv\n",
      "INFO:root:Test data uri: gs://pinhasi-superwise-vertex-demo-bucket/data/titanic_test.csv\n",
      "INFO:root:Loading csv data\n",
      "INFO:root:reading gs data: gs://pinhasi-superwise-vertex-demo-bucket/data/titanic_train.csv\n",
      "INFO:root:reading gs data: gs://pinhasi-superwise-vertex-demo-bucket/data/titanic_test.csv\n",
      "INFO:root:reading gs data: gs://pinhasi-superwise-vertex-demo-bucket/data/titanic_test.csv\n",
      "INFO:root:Defining model parameters\n",
      "INFO:root:Running feature selection\n",
      "INFO:root:Training pipelines in CV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/titanic/trainer/task.py:109: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df = df.fillna(df.mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Export trained pipeline and report\n",
      "INFO:root:f1score: 0.7919479030551312\n",
      "INFO:root:Training job completed. Exiting...\n"
     ]
    }
   ],
   "source": [
    "%run titanic/trainer/task.py $CMDARGS_LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "# create a package and upload it to the cloud bucket\n",
    "! cd titanic && python setup.py sdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_URI = f\"gs://{BUCKET_NAME}/training/trainer-0.1.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://titanic/dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  6.3 KiB/  6.3 KiB]                                                \n",
      "Operation completed over 1 objects/6.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp titanic/dist/trainer-0.1.tar.gz $PACKAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job"
   },
   "source": [
    "## Train the and deploy the model on Vertex AI\n",
    "\n",
    "Define your custom `TrainingPipeline` on Vertex AI.\n",
    "\n",
    "Use the `CustomTrainingJob` class to define the `TrainingPipeline`. The class takes the following parameters:\n",
    "\n",
    "- `display_name`: The user-defined name of this training pipeline.\n",
    "- `script_path`: The local path to the training script.\n",
    "- `container_uri`: The URI of the training container image.\n",
    "- `requirements`: The list of Python package dependencies of the script.\n",
    "- `model_serving_container_image_uri`: The URI of a container that can serve predictions for your model — either a pre-built container or a custom container.\n",
    "\n",
    "Use the `run` function to start training. The function takes the following parameters:\n",
    "\n",
    "- `args`: The command line arguments to be passed to the Python script.\n",
    "- `replica_count`: The number of worker replicas.\n",
    "- `model_display_name`: The display name of the `Model` if the script produces a managed `Model`.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "\n",
    "The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "mxIxvDdglugx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://pinhasi-superwise-vertex-demo-bucket/aiplatform-custom-training-2021-10-18-10:37:57.379 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8040198569321299968?project=1024643151155\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/1024643151155/locations/us-central1/trainingPipelines/8040198569321299968 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7353540363635654656?project=1024643151155\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/1024643151155/locations/us-central1/trainingPipelines/8040198569321299968 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/1024643151155/locations/us-central1/trainingPipelines/8040198569321299968 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/1024643151155/locations/us-central1/trainingPipelines/8040198569321299968 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/1024643151155/locations/us-central1/trainingPipelines/8040198569321299968 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob run completed. Resource name: projects/1024643151155/locations/us-central1/trainingPipelines/8040198569321299968\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/1024643151155/locations/us-central1/models/7159025761565802496\n"
     ]
    }
   ],
   "source": [
    "# Create a custom package-based training job\n",
    "JOB_NAME = 'superwise_vertex_demo_job'\n",
    "MODEL_DISPLAY_NAME = \"superwise_vertex_titanic\"\n",
    "\n",
    "\n",
    "job = aiplatform.CustomPythonPackageTrainingJob(display_name=JOB_NAME, \n",
    "                                                python_package_gcs_uri=PACKAGE_URI, \n",
    "                                                python_module_name='trainer.task', \n",
    "                                                container_uri=TRAIN_IMAGE, \n",
    "                                                model_serving_container_image_uri=DEPLOY_IMAGE, \n",
    "                                                )\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:dedicated",
    "tags": []
   },
   "source": [
    "### Deploy the model\n",
    "\n",
    "Before you use your model to make predictions, you must deploy it to an `Endpoint`. You can do this by calling the `deploy` function on the `Model` resource. This will do two things:\n",
    "\n",
    "1. Create an `Endpoint` resource for deploying the `Model` resource to.\n",
    "2. Deploy the `Model` resource to the `Endpoint` resource.\n",
    "\n",
    "\n",
    "The function takes the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "   - If only one model, then specify `{ \"0\": 100 }`, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "   - If there are existing models on the endpoint, for which the traffic will be split, then use `model_id` to specify `{ \"0\": percent, model_id: percent, ... }`, where `model_id` is the ID of an existing `DeployedModel` on the endpoint. The percentages must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
    "\n",
    "### Traffic split\n",
    "\n",
    "The `traffic_split` parameter is specified as a Python dictionary. You can deploy more than one instance of your model to an endpoint, and then set the percentage of traffic that goes to each instance.\n",
    "\n",
    "You can use a traffic split to introduce a new model gradually into production. For example, if you had one existing model in production with 100% of the traffic, you could deploy a new model to the same endpoint, direct 10% of traffic to it, and reduce the original model's traffic to 90%. This allows you to monitor the new model's performance while minimizing the distruption to the majority of users.\n",
    "\n",
    "### Compute instance scaling\n",
    "\n",
    "You can specify a single instance (or node) to serve your online prediction requests. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
    "\n",
    "If you want to use multiple nodes to serve your online prediction requests, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI autoscales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n",
    "\n",
    "### Endpoint\n",
    "\n",
    "The method will block until the model is deployed and eventually return an `Endpoint` object. If this is the first time a model is deployed to the endpoint, it may take a few additional minutes to complete provisioning of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "WMH7GrYMlugy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/1024643151155/locations/us-central1/endpoints/3108618438885507072/operations/1234668132547690496\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/1024643151155/locations/us-central1/endpoints/3108618438885507072\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/1024643151155/locations/us-central1/endpoints/3108618438885507072')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/1024643151155/locations/us-central1/endpoints/3108618438885507072\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/1024643151155/locations/us-central1/endpoints/3108618438885507072/operations/8037355369690824704\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/1024643151155/locations/us-central1/endpoints/3108618438885507072\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DEPLOYED_NAME = MODEL_DISPLAY_NAME + \"-\" + TIMESTAMP\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_count=0,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction",
    "tags": []
   },
   "source": [
    "## Make an online prediction request\n",
    "\n",
    "Send an online prediction request to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### Send the prediction request\n",
    "\n",
    "Now that you have test data, you can use it to send a prediction request. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n",
    "\n",
    "- `instances`: A list of instances for prediction. Each instance is an array of values. \n",
    "\n",
    "**Note**: The first column for each instance needs to be the record_id. We are sending this to the prediction API in order to associate it with the prediction outputs on the server side.\n",
    "\n",
    "The `predict` function returns a list, where each element in the list corresponds to the an instance in the request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "6e20473b09f5"
   },
   "outputs": [],
   "source": [
    "instances = train.to_numpy().tolist()\n",
    "predictions = endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = np.asarray(predictions.predictions, dtype= np.int)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions = 782, Total predictions = 916, Accuracy = 0.8537117903930131\n"
     ]
    }
   ],
   "source": [
    "correct = sum(y_predicted == np.array(y_train))\n",
    "accuracy = len(y_predicted)\n",
    "print(\n",
    "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Setup Superwise.ai to track your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "1. Install the Superwise Python package from pip\n",
    "2. Set environment variables with the API keys\n",
    "3. Create a Superwise client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: superwise in /opt/conda/lib/python3.7/site-packages (2.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from superwise) (2.25.1)\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (from superwise) (0.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from superwise) (1.7.1)\n",
      "Requirement already satisfied: shap in /opt/conda/lib/python3.7/site-packages (from superwise) (0.39.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from superwise) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas->superwise) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->superwise) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->superwise) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->superwise) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->superwise) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->superwise) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->superwise) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->superwise) (2021.5.30)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from shap->superwise) (0.24.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from shap->superwise) (2.0.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.7/site-packages (from shap->superwise) (0.0.7)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /opt/conda/lib/python3.7/site-packages (from shap->superwise) (4.62.3)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from shap->superwise) (0.54.0)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba->shap->superwise) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->shap->superwise) (58.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->shap->superwise) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->shap->superwise) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install superwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SUPERWISE_CLIENT_NAME='integration'\n",
      "env: SUPERWISE_SECRET='7099a035-0dd9-4c9e-84de-670b6609e7d9'\n",
      "env: SUPERWISE_CLIENT_ID='98b4af5f-a25c-4673-a59a-a7ce5dd9c232'\n"
     ]
    }
   ],
   "source": [
    "# Login to Superwise.ai portal, and click on your account icon. Click \"personal tokens\" -> \"generate tokens\" and past the values below\n",
    "%env SUPERWISE_CLIENT_NAME='integration'\n",
    "%env SUPERWISE_SECRET='7099a035-0dd9-4c9e-84de-670b6609e7d9'\n",
    "%env SUPERWISE_CLIENT_ID='98b4af5f-a25c-4673-a59a-a7ce5dd9c232'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "SuperwiseAuthException",
     "evalue": "Error get or refresh token",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSuperwiseAuthException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4143/2556240798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuperwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuperwise_enums\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaskTypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFeatureType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataTypesRoles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSuperwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/superwise/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client_id, secret, client_name, _rest_client, email, password, _fegg_url, _superwise_host)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mapi_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPERWISE_HOST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_rest_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0m_rest_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaskController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rest_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVersionController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rest_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/superwise/controller/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client_id, secret, client_name, api_host, email, password)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/superwise/controller/client.py\u001b[0m in \u001b[0;36mget_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSuperwiseAuthException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error get or refresh token url={} params={} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSuperwiseAuthException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error get or refresh token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSuperwiseAuthException\u001b[0m: Error get or refresh token"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from superwise import Superwise\n",
    "from superwise.models.task import Task\n",
    "from superwise.models.version import Version\n",
    "from superwise.models.data_entity import DataEntity\n",
    "from superwise.resources.superwise_enums import TaskTypes,FeatureType,DataTypesRoles\n",
    "\n",
    "sw = Superwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Superwise *Task*\n",
    "\n",
    "A *Task* represents a domain problem.\n",
    "In our case, the task is to predict the survival chances of the Titanic passengers.\n",
    "\n",
    "Over time, we may develop and deploy different ML models that attempt to address this task.\n",
    "\n",
    "In Superwise.ai terminology, each specific ML model we wish to track is called a *Version*. \n",
    "There may be multiple *Versions* belonging to a *Task* being tracked at any point in time (e.g. new models in shadow mode, or A/B tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:superwise:POST admin/v1/tasks \n"
     ]
    }
   ],
   "source": [
    "# Create the Task entity\n",
    "titanic_task =Task(\n",
    "    task_type=TaskTypes.BINARY_CLASSIFICATION,\n",
    "    title=\"Superwise-vertex-titanic-model\",\n",
    "    task_description=\"Predicting Titanic passengers' survival probability\",\n",
    "    monitor_delay=1)\n",
    "my_task = sw.task.create(titanic_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a *Baseline* for our deployed model\n",
    "\n",
    "We've just deployed a model to Sagemaker, and wish to start tracking it.\n",
    "In order to perform the analysis of the model's performance over time, we need to set up a Baseline for the model's behavior.\n",
    "\n",
    "It's a common practice to use the training or test data (both features and predictions)as the baseline, as they represent \n",
    "The state which we consider stable and validated.\n",
    "\n",
    "Later, when the model performs predictions in production, we can compare the data and prediction behavior to the baseline, and detect drift.\n",
    "\n",
    "The baseline data includes:\n",
    "\n",
    "1. Features\n",
    "2. Labels\n",
    "3. Model predictions\n",
    "4. Timestamp of inference\n",
    "5. Id for each record (later used to correlate predictions with labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the prediction value, a timestamp and the label to the training features\n",
    "\n",
    "baseline_data = X_train.assign(prediction=predictions.predictions,ts=pd.Timestamp.now(),survived=np.array(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>record_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>pclass</th>\n",
       "      <th>embarked</th>\n",
       "      <th>home_dest</th>\n",
       "      <th>parch</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>prediction</th>\n",
       "      <th>ts</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1214</td>\n",
       "      <td>male</td>\n",
       "      <td>29.881135</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>677</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>Bulgaria Chicago, IL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>534</td>\n",
       "      <td>female</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>Worcester, England</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1174</td>\n",
       "      <td>female</td>\n",
       "      <td>29.881135</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>864</td>\n",
       "      <td>female</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>911</td>\n",
       "      <td>1095</td>\n",
       "      <td>female</td>\n",
       "      <td>29.881135</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>3</td>\n",
       "      <td>Q</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>912</td>\n",
       "      <td>1130</td>\n",
       "      <td>female</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>913</td>\n",
       "      <td>1294</td>\n",
       "      <td>male</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>914</td>\n",
       "      <td>860</td>\n",
       "      <td>female</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>915</td>\n",
       "      <td>1126</td>\n",
       "      <td>female</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-10-18 10:50:57.623523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     record_id  record_id     sex        age     fare  pclass embarked  \\\n",
       "0            0       1214    male  29.881135   8.6625       3        S   \n",
       "1            1        677    male  26.000000   7.8958       3        S   \n",
       "2            2        534  female  19.000000  26.0000       2        S   \n",
       "3            3       1174  female  29.881135  69.5500       3        S   \n",
       "4            4        864  female  28.000000   7.7750       3        S   \n",
       "..         ...        ...     ...        ...      ...     ...      ...   \n",
       "911        911       1095  female  29.881135   7.6292       3        Q   \n",
       "912        912       1130  female  18.000000   7.7750       3        S   \n",
       "913        913       1294    male  28.500000  16.1000       3        S   \n",
       "914        914        860  female  26.000000   7.9250       3        S   \n",
       "915        915       1126  female  28.000000   7.8958       3        S   \n",
       "\n",
       "                home_dest  parch  sibsp  prediction  \\\n",
       "0                       ?      0      0         0.0   \n",
       "1    Bulgaria Chicago, IL      0      0         0.0   \n",
       "2      Worcester, England      0      0         1.0   \n",
       "3                       ?      2      8         0.0   \n",
       "4                       ?      0      0         1.0   \n",
       "..                    ...    ...    ...         ...   \n",
       "911                     ?      0      0         1.0   \n",
       "912                     ?      0      0         1.0   \n",
       "913                     ?      0      0         0.0   \n",
       "914                     ?      0      0         1.0   \n",
       "915                     ?      0      0         1.0   \n",
       "\n",
       "                            ts  survived  \n",
       "0   2021-10-18 10:50:57.623523         0  \n",
       "1   2021-10-18 10:50:57.623523         0  \n",
       "2   2021-10-18 10:50:57.623523         1  \n",
       "3   2021-10-18 10:50:57.623523         0  \n",
       "4   2021-10-18 10:50:57.623523         0  \n",
       "..                         ...       ...  \n",
       "911 2021-10-18 10:50:57.623523         0  \n",
       "912 2021-10-18 10:50:57.623523         0  \n",
       "913 2021-10-18 10:50:57.623523         0  \n",
       "914 2021-10-18 10:50:57.623523         1  \n",
       "915 2021-10-18 10:50:57.623523         0  \n",
       "\n",
       "[916 rows x 13 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a *Schema* object that describes the format and sematics of our Baseline data\n",
    "\n",
    "The Schema object helps Superwise.ai interpret our data, for example - undertand which column prepresents predictions and which represents the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = [\n",
    "     #features...\n",
    "    DataEntity(name=\"age\", type=FeatureType.NUMERIC, role=DataTypesRoles.FEATURE), \n",
    "    DataEntity(name=\"fare\", type=FeatureType.NUMERIC,  role=DataTypesRoles.FEATURE),\n",
    "    DataEntity(name=\"sex\", type=FeatureType.CATEGORICAL, role=DataTypesRoles.FEATURE),\n",
    "    DataEntity(name=\"pclass\", type=FeatureType.CATEGORICAL,  role=DataTypesRoles.FEATURE),\n",
    "    DataEntity(name=\"embarked\", type=FeatureType.CATEGORICAL, role=DataTypesRoles.FEATURE),\n",
    "    DataEntity(name=\"home_dest\", type=FeatureType.CATEGORICAL,  role=DataTypesRoles.FEATURE),\n",
    "    DataEntity(name=\"parch\", type=FeatureType.CATEGORICAL,  role=DataTypesRoles.FEATURE),\n",
    "    DataEntity(name=\"sibsp\", type=FeatureType.CATEGORICAL,  role=DataTypesRoles.FEATURE),\n",
    "    DataEntity(name=\"survived\", type=FeatureType.BOOLEAN,  role=DataTypesRoles.LABEL), # labels. When creating baseline, we use the training labels\n",
    "    DataEntity(name=\"prediction\", type=FeatureType.BOOLEAN, role=DataTypesRoles.PREDICTION_VALUE), #model predictions\n",
    "    DataEntity(name=\"ts\", type=FeatureType.TIMESTAMP,role=DataTypesRoles.TIMESTAMP), #timestamp of the inference. for baseline we use now()\n",
    "    DataEntity(name=\"record_id\", type=FeatureType.CATEGORICAL, role=DataTypesRoles.ID) #the ID field\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a *Version* object\n",
    "\n",
    "As explained above, a *Version* represents a concrete ML model we are tracking.\n",
    "\n",
    "A *Version* solves a *Task*\n",
    "\n",
    "A *Version* has a *Baseline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:superwise:GET  https://portal.superwise.ai/integration/admin/v1/tasks/37 \n",
      "INFO:superwise.controller.summary.entities_validator:start validate and prepare data\n",
      "INFO:superwise.controller.summary.entities_validator:start validate and prepare dimensions\n",
      "INFO:superwise.controller.summary.entities_validator:start validate and prepare types\n",
      "INFO:superwise.controller.summary.entities_validator:start validate and prepare roles\n",
      "INFO:superwise.controller.summary.entities_validator:Secondary type was NOT provided as part of the version entities. Computing\n",
      "INFO:superwise.controller.summary.entities_validator:Infer parquet type based on type\n",
      "DEBUG:superwise.controller.summary.entities_validator:Type map -> Unknown=string, numeric=float64, timestamp=timestamp, boolean=bool_\n",
      "INFO:superwise.controller.summary.entities_validator:Infer parquet type for categorical based on data\n",
      "DEBUG:superwise.controller.summary.entities_validator:Categorical feature sex has parquet type string\n",
      "DEBUG:superwise.controller.summary.entities_validator:Categorical feature pclass has parquet type float64\n",
      "DEBUG:superwise.controller.summary.entities_validator:Categorical feature embarked has parquet type string\n",
      "DEBUG:superwise.controller.summary.entities_validator:Categorical feature home_dest has parquet type string\n",
      "DEBUG:superwise.controller.summary.entities_validator:Categorical feature parch has parquet type float64\n",
      "DEBUG:superwise.controller.summary.entities_validator:Categorical feature sibsp has parquet type float64\n",
      "DEBUG:superwise.controller.summary.entities_validator:Categorical feature record_id has parquet type float64\n",
      "INFO:superwise:Feature importance was NOT provided as part of the version entities. Computing\n",
      "DEBUG:superwise:calc feature importance\n",
      "DEBUG:superwise:pre processing for feature importance\n",
      "DEBUG:superwise:feature importance pre pre-processing categorical feature sex\n",
      "DEBUG:superwise:create categorical embedding for sex\n",
      "DEBUG:superwise:feature importance pre pre-processing categorical feature pclass\n",
      "DEBUG:superwise:create categorical embedding for pclass\n",
      "DEBUG:superwise:feature importance pre pre-processing categorical feature embarked\n",
      "DEBUG:superwise:create categorical embedding for embarked\n",
      "DEBUG:superwise:feature importance pre pre-processing categorical feature parch\n",
      "DEBUG:superwise:create categorical embedding for parch\n",
      "DEBUG:superwise:feature importance pre pre-processing categorical feature sibsp\n",
      "DEBUG:superwise:create categorical embedding for sibsp\n",
      "DEBUG:superwise:pre-processing 2 numeric features\n",
      "DEBUG:superwise:Running parallelize shap\n",
      "DEBUG:superwise:Calculating summary based on data\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Generate summary for Numerical (Num_centered) feature age\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for age\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc range for\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc min value for age\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc max value for age\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc mean value for age\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc std value for age\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:calculated std 12.373033892829747\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc centered distribution for age\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Generate summary for Numerical (Num_right_tail) feature fare\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for fare\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc range for\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc min value for fare\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc max value for fare\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc mean value for fare\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc right tail distribution for fare\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:Calc std value for fare\n",
      "DEBUG:superwise.controller.summary.numeric_summary_generator:calculated std 52.81177508257452\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Generate summary for categorical (Cat_dense) feature sex\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for sex\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc new values for sex\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc unique values for sex\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc top frequent percent for sex\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc distribution for sex\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Generate summary for categorical (Cat_dense) feature pclass\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for pclass\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc new values for pclass\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc unique values for pclass\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc top frequent percent for pclass\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc distribution for pclass\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Generate summary for categorical (Cat_dense) feature embarked\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for embarked\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc new values for embarked\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc unique values for embarked\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc top frequent percent for embarked\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc distribution for embarked\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Generate summary for categorical (Cat_sparse) feature home_dest\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for home_dest\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Generate summary for categorical (Cat_dense) feature parch\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for parch\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc new values for parch\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc unique values for parch\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc top frequent percent for parch\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc distribution for parch\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Generate summary for categorical (Cat_dense) feature sibsp\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for sibsp\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc new values for sibsp\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc unique values for sibsp\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc top frequent percent for sibsp\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc distribution for sibsp\n",
      "DEBUG:superwise.controller.summary.boolean_summary_generator:Generate summary for boolean (Boolean_numeric) feature survived\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for survived\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc new values for survived\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc distribution for survived\n",
      "DEBUG:superwise.controller.summary.boolean_summary_generator:Generate summary for boolean (Boolean_numeric) feature prediction\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for prediction\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc new values for prediction\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Calc distribution for prediction\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc generic summary for ts\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for ts\n",
      "DEBUG:superwise.controller.summary.categorical_summary_generator:Generate summary for categorical (Cat_sparse) feature record_id\n",
      "DEBUG:superwise.controller.summary.entity_summary_generator:Calc missing values for record_id\n",
      "INFO:superwise:POST model/v1/versions \n",
      "INFO:superwise:GET  https://portal.superwise.ai/integration/model/v1/versions/17 \n",
      "INFO:superwise:finished summerizing the version, return results\n"
     ]
    }
   ],
   "source": [
    "titanic_version = Version(\n",
    "    task_id=my_task.id,\n",
    "    version_name=\"1.0\",\n",
    "    baseline_df=baseline_df,\n",
    "    data_entities=schema,\n",
    ")\n",
    "my_version = sw.version.create(titanic_version, wait_until_complete=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:superwise:PATCH model/v1/versions \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [204]>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw.version.activate(my_version.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III - monitoring ongoing predictions\n",
    "\n",
    "Now that we have a *Version* of the model setup with a *Baseline*, we can start sending ongoing model predictions to Superwise to monitor the model's performance in a production settings.\n",
    "\n",
    "For this demo, we will treat the Test split of the data as our \"ongoing predictions\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=test.to_numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>version_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>1148</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>1049</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>982</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1195</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>325</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>919</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>532</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>1159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>513</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id  task_id  version_id  prediction\n",
       "1148       1148        1           1         0.0\n",
       "1049       1049        1           1         0.0\n",
       "982         982        1           1         0.0\n",
       "808         808        1           1         0.0\n",
       "1195       1195        1           1         0.0\n",
       "...         ...      ...         ...         ...\n",
       "325         325        1           1         0.0\n",
       "919         919        1           1         0.0\n",
       "532         532        1           1         0.0\n",
       "1159       1159        1           1         1.0\n",
       "513         513        1           1         1.0\n",
       "\n",
       "[393 rows x 4 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [x for x in predictions.predictions]\n",
    "ongoing_predictions = pd.DataFrame(data=test, columns=[\"record_id\"])\n",
    "ongoing_predictions[\"task_id\"] = my_task.id\n",
    "ongoing_predictions[\"version_id\"] = my_version.id\n",
    "ongoing_predictions['prediction']=pred\n",
    "ongoing_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:superwise:file_log gs://pinhasi-superwise-vertex-demo-bucket/monitoring/test_predictions_log.csv \n",
      "INFO:superwise:file_log server response: b'{\"transaction_id\":\"09ed7964-2f18-11ec-a79c-2aae48826af7\"}'\n"
     ]
    }
   ],
   "source": [
    "# Store the model predictions in a CSV and upload it to gs\n",
    "logs = f\"gs://{BUCKET_NAME}/monitoring/test_predictions_log.csv\"\n",
    "ongoing_predictions.to_csv(logs)\n",
    "\n",
    "sw.data.log_file(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround for bucket permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional - report ongoing lables to Superwise.ai\n",
    "\n",
    "In some cases, our system is able to gather \"ground truth\" labels for it's predictions.\n",
    "Often, this happens later on, after the prediciton was already given.\n",
    "\n",
    "By sending these labels to Superwise.ai, we add another important layer of data to our monitoring solution.\n",
    "\n",
    "For the purpose of this demo, we can use the test set's labels as the ground truth, simulating a label we collected in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>survived</th>\n",
       "      <th>task_id</th>\n",
       "      <th>version_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>1148</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>1049</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>982</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>808</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1195</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>532</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>1159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>513</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id  survived  task_id  version_id\n",
       "1148       1148         0        1           1\n",
       "1049       1049         1        1           1\n",
       "982         982         0        1           1\n",
       "808         808         0        1           1\n",
       "1195       1195         0        1           1\n",
       "...         ...       ...      ...         ...\n",
       "325         325         0        1           1\n",
       "919         919         0        1           1\n",
       "532         532         0        1           1\n",
       "1159       1159         1        1           1\n",
       "513         513         1        1           1\n",
       "\n",
       "[393 rows x 4 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: we provide the column names we declared in the Schema object, so that Superwise.ai will be able to interpret the data\n",
    "\n",
    "ground_truth = pd.DataFrame(data=test, columns=['record_id', 'survived'])\n",
    "ground_truth[\"task_id\"] = my_task.id\n",
    "ground_truth[\"version_id\"] = my_task.id\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:superwise:file_log gs://pinhasi-superwise-vertex-demo-bucket/monitoring/test_labels_log.csv \n",
      "INFO:superwise:file_log server response: b'{\"transaction_id\":\"48e6ace8-2b6d-11ec-bad8-7aba68ba8419\"}'\n"
     ]
    }
   ],
   "source": [
    "# Save the lables to a .csv and store it on S3\n",
    "logs = f\"gs://{BUCKET_NAME}/monitoring/test_labels_log.csv\"\n",
    "ground_truth.to_csv(logs)\n",
    "\n",
    "sw.data.log_file(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:superwise:file_log gs://superwise-integration-production/assaf_test/labels \n",
      "INFO:superwise:file_log server response: b'{\"transaction_id\":\"19be0a92-2f19-11ec-b309-2aae48826af7\"}'\n"
     ]
    }
   ],
   "source": [
    "# workaround for bucket permissions\n",
    "ongoing_predictions.to_csv(\"version_17_labels.csv\")\n",
    "# CLI - copy to the SW bucket with the right credentials\n",
    "sw.data.log_file(\"gs://superwise-integration-production/assaf_test/labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model"
   },
   "source": [
    "## Undeploy the model\n",
    "\n",
    "To undeploy your `Model` resource from the serving `Endpoint` resource, use the endpoint's `undeploy` method with the following parameter:\n",
    "\n",
    "- `deployed_model_id`: The model deployment identifier returned by the endpoint service when the `Model` resource was deployed. You can retrieve the deployed models using the endpoint's `deployed_models` property.\n",
    "\n",
    "Since this is the only deployed model on the `Endpoint` resource, you can omit `traffic_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "khPSAO1tlug0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Undeploying Endpoint model: projects/1024643151155/locations/us-central1/endpoints/4737769215807717376\n",
      "INFO:google.cloud.aiplatform.models:Undeploy Endpoint model backing LRO: projects/1024643151155/locations/us-central1/endpoints/4737769215807717376/operations/162679479838179328\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model undeployed. Resource name: projects/1024643151155/locations/us-central1/endpoints/4737769215807717376\n"
     ]
    }
   ],
   "source": [
    "deployed_model_id = endpoint.list_models()[0].id\n",
    "endpoint.undeploy(deployed_model_id=deployed_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:custom"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Training Job\n",
    "- Model\n",
    "- Endpoint\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "NNmebHf7lug0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting CustomPythonPackageTrainingJob : projects/1024643151155/locations/us-central1/trainingPipelines/8822839742065278976\n",
      "INFO:google.cloud.aiplatform.base:Delete CustomPythonPackageTrainingJob  backing LRO: projects/1024643151155/locations/us-central1/operations/7814295246740652032\n",
      "INFO:google.cloud.aiplatform.base:CustomPythonPackageTrainingJob deleted. . Resource name: projects/1024643151155/locations/us-central1/trainingPipelines/8822839742065278976\n",
      "INFO:google.cloud.aiplatform.base:Deleting Model : projects/1024643151155/locations/us-central1/models/3161236667344355328\n",
      "INFO:google.cloud.aiplatform.base:Delete Model  backing LRO: projects/1024643151155/locations/us-central1/operations/3369242414525972480\n",
      "INFO:google.cloud.aiplatform.base:Model deleted. . Resource name: projects/1024643151155/locations/us-central1/models/3161236667344355328\n",
      "INFO:google.cloud.aiplatform.base:Deleting Endpoint : projects/1024643151155/locations/us-central1/endpoints/4737769215807717376\n",
      "INFO:google.cloud.aiplatform.base:Delete Endpoint  backing LRO: projects/1024643151155/locations/us-central1/operations/7226575495368802304\n",
      "INFO:google.cloud.aiplatform.base:Endpoint deleted. . Resource name: projects/1024643151155/locations/us-central1/endpoints/4737769215807717376\n"
     ]
    }
   ],
   "source": [
    "delete_training_job = True\n",
    "delete_model = True\n",
    "delete_endpoint = True\n",
    "\n",
    "# Warning: Setting this to true will delete everything in your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the model\n",
    "model.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "endpoint.delete()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom-tabular-bq-managed-dataset.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "common-cpu.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
